#!/usr/bin/env python
# coding: utf-8

# # Анализ лояльности пользователей Яндекс Афиши

# ## Этапы выполнения проекта
# 
# ### 1. Загрузка данных и их предобработка
# 
# ---
# 
# **Задача 1.1:** Напишите SQL-запрос, выгружающий в датафрейм pandas необходимые данные. Используйте следующие параметры для подключения к базе данных `data-analyst-afisha`:
# 
# - **Хост** — `rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net`
# - **База данных** — `data-analyst-afisha`
# - **Порт** — `6432`
# - **Аутентификация** — `Database Native`
# - **Пользователь** — `praktikum_student`
# - **Пароль** — `Sdf4$2;d-d30pp`
# 
# Для выгрузки используйте запрос из предыдущего урока и библиотеку SQLAlchemy.
# 
# Выгрузка из базы данных SQL должна позволить собрать следующие данные:
# 
# - `user_id` — уникальный идентификатор пользователя, совершившего заказ;
# - `device_type_canonical` — тип устройства, с которого был оформлен заказ (`mobile` — мобильные устройства, `desktop` — стационарные);
# - `order_id` — уникальный идентификатор заказа;
# - `order_dt` — дата создания заказа (используйте данные `created_dt_msk`);
# - `order_ts` — дата и время создания заказа (используйте данные `created_ts_msk`);
# - `currency_code` — валюта оплаты;
# - `revenue` — выручка от заказа;
# - `tickets_count` — количество купленных билетов;
# - `days_since_prev` — количество дней от предыдущей покупки пользователя, для пользователей с одной покупкой — значение пропущено;
# - `event_id` — уникальный идентификатор мероприятия;
# - `service_name` — название билетного оператора;
# - `event_type_main` — основной тип мероприятия (театральная постановка, концерт и так далее);
# - `region_name` — название региона, в котором прошло мероприятие;
# - `city_name` — название города, в котором прошло мероприятие.
# 
# ---
# 

# In[1]:


get_ipython().system('pip install sqlalchemy')
get_ipython().system('pip install psycopg2-binary')
get_ipython().system('pip install phik')
import pandas as pd
from sqlalchemy import create_engine

# Загружаем библиотеки для визуализации данных
import matplotlib.pyplot as plt
import seaborn as sns

# Загружаем библиотеку для расчёта коэффициента корреляции phi_k
from phik import phik_matrix


# In[2]:


db_config = {'user': 'praktikum_student', # имя пользователя
             'pwd': 'Sdf4$2;d-d30pp', # пароль
             'host': 'rc1b-wcoijxj3yxfsf3fs.mdb.yandexcloud.net',
             'port': 6432, # порт подключения
             'db': 'data-analyst-afisha' # название базы данных
             }
connection_string = 'postgresql://{}:{}@{}:{}/{}'.format(
    db_config['user'],
    db_config['pwd'],
    db_config['host'],
    db_config['port'],
    db_config['db'],
)
engine = create_engine(connection_string)


# In[3]:


query = '''
WITH set_config_precode AS (
  SELECT set_config('synchronize_seqscans', 'off', true)
)
SELECT user_id,
       device_type_canonical,
       order_id,
       created_dt_msk as order_dt,
       created_ts_msk as order_ts,
       currency_code,
       revenue,
       tickets_count,
       days_since_prev,
       event_id,
       service_name,
       event_type_main,
       region_name,
       city_name
FROM(
SELECT user_id,
       device_type_canonical,
       order_id,
       created_dt_msk,
       created_ts_msk,
       currency_code,
       revenue,
       tickets_count,
       event_id,
       event_name_code,
       service_name,
       event_type_main,
       region_name,
       city_name,
       LAG(created_dt_msk,1) OVER (PARTITION BY user_id ORDER BY created_dt_msk) as drt,
       created_dt_msk::date - LAG(created_dt_msk::date) OVER (PARTITION BY user_id ORDER BY created_dt_msk) as days_since_prev
FROM afisha.purchases
LEFT JOIN afisha.events USING (event_id)
LEFT JOIN afisha.city USING (city_id)
LEFT JOIN afisha.regions USING (region_id)
WHERE (device_type_canonical = 'mobile' or device_type_canonical = 'desktop') AND event_type_main != 'фильм'
ORDER BY user_id) as ttt
'''
df = pd.read_sql_query(query, con=engine)


# ---
# 
# **Задача 1.2:** Изучите общую информацию о выгруженных данных. Оцените корректность выгрузки и объём полученных данных.
# 
# Предположите, какие шаги необходимо сделать на стадии предобработки данных — например, скорректировать типы данных.
# 
# Зафиксируйте основную информацию о данных в кратком промежуточном выводе.
# 
# ---

# In[4]:


print(df.info())


# In[5]:


print(df.head())


# **Промежуточный вывод 1.2**
# 
# Всего в данных 14 столбцов и 290 611 строк. Пропуски есть только в столбце `days_since_prev` - это связано с тем, что не все пользователи совершали повторные покупки и для первой покупки значение отсутствует.

# **Формат данных**
# 
# 1) `order_dt` - представлен datetime64[ns]. Можно скорректировать `order_dt` - до дней
# 
# 2) `order_id` и `event_id` - это идентификаторы и числовых операций не будет, поэтому исправление на формат 'object'
# 
# 3) размерность всех int64 и float64 уменьшить

# ---
# 
# ###  2. Предобработка данных
# 
# Выполните все стандартные действия по предобработке данных:
# 
# ---
# 
# **Задача 2.1:** Данные о выручке сервиса представлены в российских рублях и казахстанских тенге. Приведите выручку к единой валюте — российскому рублю.
# 
# Для этого используйте датасет с информацией о курсе казахстанского тенге по отношению к российскому рублю за 2024 год — `final_tickets_tenge_df.csv`. Его можно загрузить по пути `https://code.s3.yandex.net/datasets/final_tickets_tenge_df.csv')`
# 
# Значения в рублях представлено для 100 тенге.
# 
# Результаты преобразования сохраните в новый столбец `revenue_rub`.
# 
# ---
# 

# Выгрузка данных, изменение формата в дату для дальнейшего присоедения

# In[6]:


tenge_df = pd.read_csv('https://code.s3.yandex.net/datasets/final_tickets_tenge_df.csv')


# In[7]:


tenge_df['data']=pd.to_datetime(tenge_df['data'])
print(tenge_df.info())
print(tenge_df.head())


# Присоединение датафреймов с входной и выходной проверкой количества данных

# In[8]:


temp = df.copy() 
len(temp)


# In[9]:


df_df = pd.merge(df, tenge_df, left_on='order_dt',right_on='data', how='left')


# In[10]:


temp = df_df.copy() 
len(temp)


# Перевод валюты по отношению к дате совершения сделки

# In[11]:


df_df['revenue_rub'] = df_df.apply(lambda row: row['revenue'] if row['currency_code'] == 'rub' else row['revenue']/100*row['curs'], axis=1)


# Удаление лишних столбцов и проверка

# In[12]:


df = df_df.drop(['data', 'nominal', 'curs', 'cdx'], axis=1)


# In[13]:


print(df[df['currency_code'] != 'rub'].head())


# ---
# 
# **Задача 2.2:**
# 
# - Проверьте данные на пропущенные значения. Если выгрузка из SQL была успешной, то пропуски должны быть только в столбце `days_since_prev`.
# - Преобразуйте типы данных в некоторых столбцах, если это необходимо. Обратите внимание на данные с датой и временем, а также на числовые данные, размерность которых можно сократить.
# - Изучите значения в ключевых столбцах. Обработайте ошибки, если обнаружите их.
#     - Проверьте, какие категории указаны в столбцах с номинальными данными. Есть ли среди категорий такие, что обозначают пропуски в данных или отсутствие информации? Проведите нормализацию данных, если это необходимо.
#     - Проверьте распределение численных данных и наличие в них выбросов. Для этого используйте статистические показатели, гистограммы распределения значений или диаграммы размаха.
#         
#         Важные показатели в рамках поставленной задачи — это выручка с заказа (`revenue_rub`) и количество билетов в заказе (`tickets_count`), поэтому в первую очередь проверьте данные в этих столбцах.
#         
#         Если обнаружите выбросы в поле `revenue_rub`, то отфильтруйте значения по 99 перцентилю.
# 
# После предобработки проверьте, были ли отфильтрованы данные. Если были, то оцените, в каком объёме. Сформулируйте промежуточный вывод, зафиксировав основные действия и описания новых столбцов.
# 
# ---

# Проверка пропусков

# In[14]:


df.isna().sum()


# In[15]:


df.isna().sum() / df.shape[0]


# Корректировка типов данных (наносекунды я пытался сократить, данные превращались в object, поэтому переделывать не стал)

# In[16]:


df.dtypes


# In[17]:


for column in ['tickets_count']:
    df[column] = pd.to_numeric(df[column], downcast='integer')

for column in ['revenue','revenue_rub','days_since_prev']:
    df[column] = pd.to_numeric(df[column], downcast='float')
    
df['order_id'] = df['order_id'].astype(object)
df['event_id'] = df['event_id'].astype(object)


# In[18]:


df.dtypes


# Вывод уникальных значений в категориях

# In[19]:


df['device_type_canonical'].unique()


# In[20]:


df['currency_code'].unique()


# In[21]:


df['event_type_main'].unique()


# Вывод статистических значений и отчистка от выбросов

# In[22]:


df['revenue_rub'].describe()


# In[23]:


# Создаём контейнер графика matplotlib и задаём его размер
plt.figure(figsize=(7, 2))

# Строим диаграмму размаха значений в столбце score
df.boxplot(column='revenue_rub', vert=False)

# Добавляем заголовок и метки оси
plt.title('Распределение выручки в рублях')
plt.xlabel('Общий чек')

# Выводим график
plt.show()


# In[24]:


revenue_rub_99=df['revenue_rub'].quantile(0.99)
print(revenue_rub_99)


# In[25]:


df['tickets_count'].describe()


# In[26]:


# Создаём контейнер графика matplotlib и задаём его размер
plt.figure(figsize=(7, 2))

# Строим диаграмму размаха значений в столбце score
df.boxplot(column='tickets_count', vert=False)

# Добавляем заголовок и метки оси
plt.title('Распределение количества билетов в заказе')
plt.xlabel('Количество билетов в заказе')

# Выводим график
plt.show()


# In[27]:


tickets_count_99=df['tickets_count'].quantile(0.99)
print(tickets_count_99)


# In[28]:


new_df = df.loc[(df['revenue_rub'] <= revenue_rub_99) & (df['tickets_count'] <= tickets_count_99)]


# In[29]:


new_df['revenue_rub'].describe()


# In[30]:


new_df['tickets_count'].describe()


# In[31]:


temp2 = new_df.copy() 
len(temp2)


# In[32]:


poterya=len(temp)-len(temp2)
skolko=poterya/len(temp)
print(poterya,skolko)


# In[33]:


print(new_df.info())


# **Вывод предобработки данных.**
# 
# Были пробразованы типы данных столбцов:
# `order_dt`, `event_id` - в object
# `revenue`, `tickets_count`, `days_since_prev` - уменьшена размерность
# 
# `revenue_rub` - добавлен столбец с выручкой в рублях (тенге переведены по курсу на день совершения сделки)
# 
# `revenue_rub`, `tickets_count` - проверены статистические данные, произошла фильтрация по 99 процентилю из-за сильных выбросов влияющих на общую статистику
# Эти данные составили 1% от общего числа строк.
# 
# Данные сохранены в `new_df` 

# ---
# 
# ### 3. Создание профиля пользователя
# 
# В будущем отдел маркетинга планирует создать модель для прогнозирования возврата пользователей. Поэтому сейчас они просят вас построить агрегированные признаки, описывающие поведение и профиль каждого пользователя.
# 
# ---
# 
# **Задача 3.1.** Постройте профиль пользователя — для каждого пользователя найдите:
# 
# - дату первого и последнего заказа;
# - устройство, с которого был сделан первый заказ;
# - регион, в котором был сделан первый заказ;
# - билетного партнёра, к которому обращались при первом заказе;
# - жанр первого посещённого мероприятия (используйте поле `event_type_main`);
# - общее количество заказов;
# - средняя выручка с одного заказа в рублях;
# - среднее количество билетов в заказе;
# - среднее время между заказами.
# 
# После этого добавьте два бинарных признака:
# 
# - `is_two` — совершил ли пользователь 2 и более заказа;
# - `is_five` — совершил ли пользователь 5 и более заказов.
# 
# **Рекомендация:** перед тем как строить профиль, отсортируйте данные по времени совершения заказа.
# 
# ---
# 

# In[34]:


value_counts = new_df['user_id'].value_counts()
print(value_counts)


# In[35]:


grouped_df = new_df.groupby('user_id').agg(
    first_order=('order_ts', 'min'),
    last_order=('order_ts', 'max'),
    order_count=('order_id', 'count'),
    avg_revenue=('revenue_rub', 'mean'),
    avg_tickets=('tickets_count', 'mean'),
    avg_days=('days_since_prev', 'mean')
).reset_index()


# In[36]:


temp = grouped_df.copy() 
len(temp)


# In[37]:


new_df2 = pd.merge(grouped_df, new_df, left_on=['user_id', 'first_order'], right_on=['user_id', 'order_ts'], how='left')


# In[38]:


temp = new_df2.copy() 
len(temp)


# In[39]:


predfinal_df = new_df2[['user_id', 'first_order', 'last_order', 'device_type_canonical', 'region_name', 'service_name', 'event_type_main', 'order_count', 'avg_revenue', 'avg_tickets','avg_days']].copy()


# In[40]:


y=0
def create_is_y(x):
    if x>y:
        return 1
    return 0


# In[41]:


y=1
predfinal_df['is_two'] = predfinal_df['order_count'].apply(create_is_y)


# In[42]:


y=4
predfinal_df['is_five'] = predfinal_df['order_count'].apply(create_is_y)


# In[43]:


print(predfinal_df.head())


# ---
# 
# **Задача 3.2.** Прежде чем проводить исследовательский анализ данных и делать выводы, важно понять, с какими данными вы работаете: насколько они репрезентативны и нет ли в них аномалий.
# 
# Используя данные о профилях пользователей, рассчитайте:
# 
# - общее число пользователей в выборке;
# - среднюю выручку с одного заказа;
# - долю пользователей, совершивших 2 и более заказа;
# - долю пользователей, совершивших 5 и более заказов.
# 
# Также изучите статистические показатели:
# 
# - по общему числу заказов;
# - по среднему числу билетов в заказе;
# - по среднему количеству дней между покупками.
# 
# По результатам оцените данные: достаточно ли их по объёму, есть ли аномальные значения в данных о количестве заказов и среднем количестве билетов?
# 
# Если вы найдёте аномальные значения, опишите их и примите обоснованное решение о том, как с ними поступить:
# 
# - Оставить и учитывать их при анализе?
# - Отфильтровать данные по какому-то значению, например, по 95-му или 99-му перцентилю?
# 
# Если вы проведёте фильтрацию, то вычислите объём отфильтрованных данных и выведите статистические показатели по обновлённому датасету.

# In[44]:


value_counts = predfinal_df['user_id'].value_counts()
print(f'общее число пользователей в выборке: {value_counts.count()}')


# In[45]:


value_mean = predfinal_df['avg_revenue'].mean()
print(f'средняя выручка с одного заказа: {round(value_mean,2)}')


# In[46]:


is_two_mean = predfinal_df['is_two'].mean()
print(f'доля пользователей, совершивших 2 и более заказа: {round(is_two_mean,2)}')


# In[47]:


is_five_mean = predfinal_df['is_five'].mean()
print(f'доля пользователей, совершивших 5 и более заказов: {round(is_five_mean,2)}')


# In[48]:


temp_predfinal = predfinal_df.copy() 
len(temp_predfinal)


# In[49]:


predfinal_df[['order_count','avg_days','avg_tickets','avg_revenue']].describe()


# In[50]:


# Создаём контейнер графика matplotlib и задаём его размер
plt.figure(figsize=(10, 2))

# Строим диаграмму размаха значений в столбце score
predfinal_df.boxplot(column='order_count', vert=False)

# Добавляем заголовок и метки оси
plt.title('Распределение количества заказов')
plt.xlabel('Количество заказов')

# Выводим график
plt.show()


# In[51]:


count_95=predfinal_df['order_count'].quantile(0.95)
print(count_95)


# In[52]:


# Создаём контейнер графика matplotlib и задаём его размер
plt.figure(figsize=(10, 2))

# Строим диаграмму размаха значений в столбце score
predfinal_df.boxplot(column='avg_tickets', vert=False)

# Добавляем заголовок и метки оси
plt.title('Распределение среднего количества билетов в заказе')
plt.xlabel('Среднее количество билетов в заказе')

# Выводим график
plt.show()


# In[53]:


tickets_99=predfinal_df['avg_tickets'].quantile(0.99)
print(tickets_99)


# In[54]:


# Создаём контейнер графика matplotlib и задаём его размер
plt.figure(figsize=(10, 2))

# Строим диаграмму размаха значений в столбце score
predfinal_df.boxplot(column='avg_days', vert=False)

# Добавляем заголовок и метки оси
plt.title('Распределение среднего количества дней между заказами')
plt.xlabel('Среднее количество дней между заказами')

# Выводим график
plt.show()


# In[55]:


plt.figure(figsize=(10, 5))

predfinal_df['avg_days'].plot(
                kind='hist',
                bins=40,
                alpha=0.75,
                edgecolor='black',
                rot=0,
    
)

# Настраиваем оформление графика
plt.title('Распределение среднего количества дней между заказами')
plt.xlabel('Среднее количество дней между заказами')
plt.ylabel('Частота')
# Добавляем сетку графика
plt.grid()

# Выводим график
plt.show()


# In[56]:


days_99=predfinal_df['avg_days'].quantile(0.99)
print(days_99)


# Данных достаточно по объёму
# 
# В среднем количестве билетов в заказе аномалий не обнаружено, в среднем количестве дней между заказами может показаться, что в данных также есть аномалии, но распределение похоже на нормальное.
# 
# В количестве заказов обнаружено анамально большие значения. Принято решение отфильтровать по 95-му перцентилю. 

# In[57]:


final_df = predfinal_df.loc[predfinal_df['order_count'] < count_95]


# In[58]:


temp_final = final_df.copy() 
len(temp_final)


# In[59]:


(len(temp_predfinal)-len(temp_final))/len(temp_predfinal)


# Итоговый набор данных был сокращен на 5%, что составляет 20594 значений.

# In[60]:


final_df[['order_count','avg_days','avg_tickets','avg_revenue']].describe()


# ---
# 
# ### 4. Исследовательский анализ данных
# 
# Следующий этап — исследование признаков, влияющих на возврат пользователей, то есть на совершение повторного заказа. Для этого используйте профили пользователей.

# 
# 
# #### 4.1. Исследование признаков первого заказа и их связи с возвращением на платформу
# 
# Исследуйте признаки, описывающие первый заказ пользователя, и выясните, влияют ли они на вероятность возвращения пользователя.
# 
# ---
# 
# **Задача 4.1.1.** Изучите распределение пользователей по признакам.
# 
# - Сгруппируйте пользователей:
#     - по типу их первого мероприятия;
#     - по типу устройства, с которого совершена первая покупка;
#     - по региону проведения мероприятия из первого заказа;
#     - по билетному оператору, продавшему билеты на первый заказ.
# - Подсчитайте общее количество пользователей в каждом сегменте и их долю в разрезе каждого признака. Сегмент — это группа пользователей, объединённых определённым признаком, то есть объединённые принадлежностью к категории. Например, все клиенты, сделавшие первый заказ с мобильного телефона, — это сегмент.
# - Ответьте на вопрос: равномерно ли распределены пользователи по сегментам или есть выраженные «точки входа» — сегменты с наибольшим числом пользователей?
# 
# ---
# 

# In[61]:


colich_event_type_main=final_df['event_type_main'].value_counts()
print(f'Распределение количества по типам мероприятий:\n{colich_event_type_main}')
dolya_event_type_main=final_df['event_type_main'].value_counts()/len(final_df)
print(f'Доля по типам мероприятий:\n{dolya_event_type_main}')


# In[62]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
colich_event_type_main.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=45, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение количества по типам мероприятий'
)

# Настраиваем оформление графика
plt.xlabel('Тип мероприятия')
plt.ylabel('Количество')

# Выводим график
plt.show()


# In[63]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_event_type_main.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по типам мероприятий'
)

# Настраиваем оформление графика
plt.xlabel('Тип мероприятия')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[64]:


colich_device_type_canonical=final_df['device_type_canonical'].value_counts()
print(f'Распределение количества по типам устройств:\n{colich_device_type_canonical}')
dolya_device_type_canonical=final_df['device_type_canonical'].value_counts()/len(final_df)
print(f'Доля по типам устройств:\n{dolya_device_type_canonical}')


# In[65]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
colich_device_type_canonical.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение количества по типам устройств'
)

# Настраиваем оформление графика
plt.xlabel('Тип устройств')
plt.ylabel('Количество')

# Выводим график
plt.show()


# In[66]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_device_type_canonical.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по типам устройств'
)

# Настраиваем оформление графика
plt.xlabel('Тип устройства')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[67]:


colich_region_name=final_df['region_name'].value_counts()
print(f'Распределение количества по типам мероприятий:\n{colich_region_name}')
dolya_region_name=final_df['region_name'].value_counts()/len(final_df)
print(f'Доля по типам мероприятий:\n{dolya_region_name}')


# In[68]:


plt.figure(figsize=(15, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
colich_region_name.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение количества по регионам'
)

# Настраиваем оформление графика
plt.xlabel('Регион')
plt.ylabel('Количество')

# Выводим график
plt.show()


# In[69]:


plt.figure(figsize=(15, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_region_name.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Доля по регионам'
)

# Настраиваем оформление графика
plt.xlabel('Регион')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[70]:


colich_service_name=final_df['service_name'].value_counts()
print(f'Распределение количества по билетному оператору:\n{colich_service_name}')
dolya_service_name=final_df['service_name'].value_counts()/len(final_df)
print(f'Доля по билетному оператору:\n{dolya_service_name}')


# In[71]:


plt.figure(figsize=(13, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
colich_service_name.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение количества по билетному оператору'
)

# Настраиваем оформление графика
plt.xlabel('Билетный оператор')
plt.ylabel('Количество')

# Выводим график
plt.show()


# In[72]:


plt.figure(figsize=(15, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_service_name.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Доля по билетному оператору'
)

# Настраиваем оформление графика
plt.xlabel('Билетный оператор')
plt.ylabel('Доля')

# Выводим график
plt.show()


# *По типам мероприятий*
# 
# На концерт, другое и театр больше всего первых покупок. 
# 
# Концерт 44.4%, Театр 19.6%. 
# 
# Остальные Стендап, Спорт, Выставки и Елки вместе взятые - это около 10% от общего числа
# 
# *По типам устройств*
# 
# Мобильные заказы явный лидер - 83 %, ПК - 17 %
# 
# *По регионам*
# 
# Выделяются Каменевский регион 33% и Североярская область 17% остальные 6% и ниже
# 
# *По билетному оператору*
# 
# Пятерка лидеров
# 1) Билеты без проблем        23%
# 2) Мой билет                 14%
# 3) Лови билет!               13%
# 4) Билеты в руки             11%
# 5) Облачко                   10%
# 
# У других меньше 5%
# 

# ---
# 
# **Задача 4.1.2.** Проанализируйте возвраты пользователей:
# 
# - Для каждого сегмента вычислите долю пользователей, совершивших два и более заказа.
# - Визуализируйте результат подходящим графиком. Если сегментов слишком много, то поместите на график только 10 сегментов с наибольшим количеством пользователей. Такое возможно с сегментами по региону и по билетному оператору.
# - Ответьте на вопросы:
#     - Какие сегменты пользователей чаще возвращаются на Яндекс Афишу?
#     - Наблюдаются ли успешные «точки входа» — такие сегменты, в которых пользователи чаще совершают повторный заказ, чем в среднем по выборке?
# 
# При интерпретации результатов учитывайте размер сегментов: если в сегменте мало пользователей (например, десятки), то доли могут быть нестабильными и недостоверными, то есть показывать широкую вариацию значений.
# 
# ---
# 

# In[73]:


final_df_2=final_df[final_df['is_two']>0].copy()


# In[74]:


len(final_df)


# In[75]:


colich_event_type_main=final_df_2['event_type_main'].value_counts()
print(f'Распределение количества по типам мероприятий:\n{colich_event_type_main}')
dolya_event_type_main=final_df_2['event_type_main'].value_counts()/final_df['event_type_main'].value_counts()
print(f'Доля по типам мероприятий:\n{dolya_event_type_main.sort_values(ascending=False)}')


# In[76]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_event_type_main.sort_values(ascending=False).plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по типам мероприятий, два и более заказа'
)

mean_churn_share = dolya_event_type_main.mean()

# Наносим на график линию с средним значением доли нелояльных клиентов
plt.axhline(mean_churn_share, # Данные, по которым строится линия
            color='red', # Цвет линии
            linestyle='--', # Стиль линии
            linewidth=1, # Ширина линии
            label=f'средний рейтинг {round(mean_churn_share,4)}')

# Настраиваем оформление графика
plt.xlabel('Тип мероприятия')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[77]:


colich_device_type_canonical=final_df_2['device_type_canonical'].value_counts()
print(f'Распределение количества по типам устройств:\n{colich_device_type_canonical}')
dolya_device_type_canonical=final_df_2['device_type_canonical'].value_counts()/final_df['device_type_canonical'].value_counts()
print(f'Доля по типам устройств:\n{dolya_device_type_canonical.sort_values(ascending=False)}')


# In[78]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
dolya_device_type_canonical.sort_values(ascending=False).plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по типам устройств, два и более заказа'
)

# Настраиваем оформление графика
plt.xlabel('Тип устройства')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[79]:


colich_region_name=final_df_2['region_name'].value_counts()
print(f'Распределение количества по типам мероприятий:\n{colich_region_name.sort_values(ascending=False)[:10]}')
dolya_region_name=final_df_2['region_name'].value_counts()/final_df['region_name'].value_counts()
print(f'Доля по типам мероприятий:\n{dolya_region_name.sort_values(ascending=False)[:10]}')


# In[80]:


df_region = pd.DataFrame({
'count': colich_region_name,
'dolya': dolya_region_name
})
df_top_region = df_region.sort_values('count', ascending=False).head(10)
print(df_top_region)


# In[81]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
df_top_region['dolya'].plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по топ 10 регионов, два и более заказа'
)
mean_churn_share = df_top_region['dolya'].mean()

# Наносим на график линию с средним значением 
plt.axhline(mean_churn_share, # Данные, по которым строится линия
            color='red', # Цвет линии
            linestyle='--', # Стиль линии
            linewidth=1, # Ширина линии
            )

# Настраиваем оформление графика
plt.xlabel('Регион')
plt.ylabel('Доля')

# Выводим график
plt.show()


# In[82]:


colich_service_name=final_df['service_name'].value_counts()
print(f'Распределение количества по билетному оператору:\n{colich_service_name.sort_values(ascending=False)[:10]}')
dolya_service_name=final_df_2['service_name'].value_counts()/final_df['service_name'].value_counts()
print(f'Доля по билетному оператору:\n{dolya_service_name.sort_values(ascending=False)}')


# In[83]:


df_service_name = pd.DataFrame({
'count': colich_service_name,
'dolya': dolya_service_name
})
df_top_service_name = df_service_name.sort_values('count', ascending=False).head(10)
print(df_top_service_name )


# In[84]:


plt.figure(figsize=(7, 3))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
df_top_service_name['dolya'].plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=90, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли по билетному оператору, два и более заказа'
)
mean_churn_share = df_top_region['dolya'].mean()

# Наносим на график линию с средним значением 
plt.axhline(mean_churn_share, # Данные, по которым строится линия
            color='red', # Цвет линии
            linestyle='--', # Стиль линии
            linewidth=1, # Ширина линии
            )

# Настраиваем оформление графика
plt.xlabel('Оператор')
plt.ylabel('Доля')

# Выводим график
plt.show()


# *По типам мероприятий*
# 
# Выше среднего показателя выставки 63% и театр 62%
# 
# *По типам устройств*
# 
# ПК - 62% против мобильных - 59%
# 
# *По регионам*
# 
# Особенно выделяются Широковская область 63%, Шанырский регион 65%, Светополянский округ 64%
# 
# *По билетному оператору*
# 
# Край билетов 63%, Дом культуры 63%
# 

# ---
# 
# **Задача 4.1.3.** Опираясь на выводы из задач выше, проверьте продуктовые гипотезы:
# 
# - **Гипотеза 1.** Тип мероприятия влияет на вероятность возврата на Яндекс Афишу: пользователи, которые совершили первый заказ на спортивные мероприятия, совершают повторный заказ чаще, чем пользователи, оформившие свой первый заказ на концерты.
# - **Гипотеза 2.** В регионах, где больше всего пользователей посещают мероприятия, выше доля повторных заказов, чем в менее активных регионах.
# 
# ---

# Гипотеза 1. Тип мероприятия влияет на вероятность возврата на Яндекс Афишу: пользователи, которые совершили первый заказ на спортивные мероприятия, совершают повторный заказ чаще, чем пользователи, оформившие свой первый заказ на концерты.
# 
# **Не подтверждается**
# 
# спорт 53% вернувшихся(ниже среднего), концерты 60% вернувшихся(средний показатель)
# 

# Гипотеза 2. В регионах, где больше всего пользователей посещают мероприятия, выше доля повторных заказов, чем в менее активных регионах.
# 
# **Не подтверждается**
# 
# Топовые по посещению регионы Каменевский регион 60%, Североярская область 62%, в то время как в других менее активных есть показатели до 65% 

# ---
# 
# #### 4.2. Исследование поведения пользователей через показатели выручки и состава заказа
# 
# Изучите количественные характеристики заказов пользователей, чтобы узнать среднюю выручку сервиса с заказа и количество билетов, которое пользователи обычно покупают.
# 
# Эти метрики важны не только для оценки выручки, но и для оценки вовлечённости пользователей. Возможно, пользователи с более крупными и дорогими заказами более заинтересованы в сервисе и поэтому чаще возвращаются.
# 
# ---
# 
# **Задача 4.2.1.** Проследите связь между средней выручкой сервиса с заказа и повторными заказами.
# 
# - Постройте сравнительные гистограммы распределения средней выручки с билета (`avg_revenue_rub`):
#     - для пользователей, совершивших один заказ;
#     - для вернувшихся пользователей, совершивших 2 и более заказа.
# - Ответьте на вопросы:
#     - В каких диапазонах средней выручки концентрируются пользователи из каждой группы?
#     - Есть ли различия между группами?
# 
# Текст на сером фоне:
#     
# **Рекомендация:**
# 
# 1. Используйте одинаковые интервалы (`bins`) и прозрачность (`alpha`), чтобы визуально сопоставить распределения.
# 2. Задайте параметру `density` значение `True`, чтобы сравнивать форму распределений, даже если число пользователей в группах отличается.
# 
# ---
# 

# In[85]:


plt.figure(figsize=(12, 4))

# Строим гистограммы для каждого значения churn
for i in final_df['is_two'].unique():
    # Фильтруем данные по значению столбца churn
    final_df.loc[final_df['is_two'] == i, 'avg_revenue'].plot(
        kind='hist',
        bins=25,
        alpha=0.5,
        label=f'{i}',
        legend=True,
        density=True
    )

# Настраиваем внешний вид графика и выводим его на экран
plt.title(f'Сравнение распределение среднего чека в зависимости от признака"is_two"')
plt.xlabel('Средний чек')
plt.ylabel('Количество')
plt.legend(title='Значение is_two')
plt.show() 


# Для тех кто вернулся средний чек выше в области от 300 до 900 руб, в остльных областях выше у тех, кто совершил только один заказ.
# 
# Больше всего пользователей совершивших один заказ в области 0-200, далее постепенное снижение.
# Больше всего пользователей совершивших повторный заказ в области 300-500.

# ---
# 
# **Задача 4.2.2.** Сравните распределение по средней выручке с заказа в двух группах пользователей:
# 
# - совершившие 2–4 заказа;
# - совершившие 5 и более заказов.
# 
# Ответьте на вопрос: есть ли различия по значению средней выручки с заказа между пользователями этих двух групп?
# 
# ---
# 

# In[86]:


new_df_five = final_df.loc[(final_df['is_two'] > 0)]

plt.figure(figsize=(12, 4))

# Строим гистограммы для каждого значения churn
for i in new_df_five['is_five'].unique():
    # Фильтруем данные по значению столбца churn
    new_df_five.loc[new_df_five['is_five'] == i, 'avg_revenue'].plot(
        kind='hist',
        bins=22,
        alpha=0.5,
        label=f'{i}',
        legend=True,
        density=True
    )

# Настраиваем внешний вид графика и выводим его на экран
plt.title(f'Сравнение распределение среднего чека в зависимости от признака"is_five"')
plt.xlabel('Средний чек')
plt.ylabel('Количество')
plt.legend(title='Значение is_five')
plt.show()


# In[87]:


new_df_five['avg_revenue'].hist(figsize=(5, 5), bins=25)


# Для тех кто вернулся более 5 раз средний чек также выше в области от 300 до 800 руб, в остльных областях выше у тех, кто совершил 2-4 заказа.
# 
# Больше всего пользователей совершивших 2-4 заказа в области 0-200.
# Больше всего пользователей совершивших повторный заказ более 5 раз в области 300-700.

# ---
# 
# **Задача 4.2.3.** Проанализируйте влияние среднего количества билетов в заказе на вероятность повторной покупки.
# 
# - Изучите распределение пользователей по среднему количеству билетов в заказе (`avg_tickets_count`) и опишите основные наблюдения.
# - Разделите пользователей на несколько сегментов по среднему количеству билетов в заказе:
#     - от 1 до 2 билетов;
#     - от 2 до 3 билетов;
#     - от 3 до 5 билетов;
#     - от 5 и более билетов.
# - Для каждого сегмента подсчитайте общее число пользователей и долю пользователей, совершивших повторные заказы.
# - Ответьте на вопросы:
#     - Как распределены пользователи по сегментам — равномерно или сконцентрировано?
#     - Есть ли сегменты с аномально высокой или низкой долей повторных покупок?
# 
# ---

# In[88]:


new_df_avg_2 = final_df.loc[(final_df['avg_tickets'] >= 1) & (final_df['avg_tickets'] < 2)]
len(new_df_avg_2)


# In[89]:


new_df_avg_2['is_two'].mean()


# In[90]:


new_df_avg_3 = final_df.loc[(final_df['avg_tickets'] >= 2) & (final_df['avg_tickets'] < 3)]
len(new_df_avg_3)


# In[91]:


new_df_avg_3['is_two'].mean()


# In[92]:


new_df_avg_5 = final_df.loc[(final_df['avg_tickets'] >= 3) & (final_df['avg_tickets'] < 5)]
len(new_df_avg_5)


# In[93]:


new_df_avg_5['is_two'].mean()


# In[94]:


new_df_avg_b5 = final_df.loc[(final_df['avg_tickets'] >=5)]
len(new_df_avg_b5)


# In[95]:


new_df_avg_b5['is_two'].mean()


# Пользователи по сегментам распределены не равномерно, больше всего пользователей от 2 до 5 билетов в заказе.
# 
# Очень большой показатель повторных покупок у пользователей с билетами >=2 и <3 билетов в заказе.
# 
# Очень низкий показатель 5 и более.

# ---
# 
# #### 4.3. Исследование временных характеристик первого заказа и их влияния на повторные покупки
# 
# Изучите временные параметры, связанные с первым заказом пользователей:
# 
# - день недели первой покупки;
# - время с момента первой покупки — лайфтайм;
# - средний интервал между покупками пользователей с повторными заказами.
# 
# ---
# 
# **Задача 4.3.1.** Проанализируйте, как день недели, в которой была совершена первая покупка, влияет на поведение пользователей.
# 
# - По данным даты первого заказа выделите день недели.
# - Для каждого дня недели подсчитайте общее число пользователей и долю пользователей, совершивших повторные заказы. Результаты визуализируйте.
# - Ответьте на вопрос: влияет ли день недели, в которую совершена первая покупка, на вероятность возврата клиента?
# 
# ---
# 

# In[96]:


final = final_df.copy()

final['day_week'] = final['first_order'].dt.weekday


# In[97]:


colich_day_week=final['day_week'].value_counts()
print(f'Распределение количества по билетному оператору:\n{colich_day_week}')


# In[98]:


plt.figure(figsize=(13, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
colich_day_week.plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение количества по дням недели'
)

# Настраиваем оформление графика
plt.xlabel('День недели(0 - пн, 1 - вт, 2 - ср, 3 - чт, 4- пт, 5 - сб, 6 - вск)')
plt.ylabel('Количество')

# Выводим график
plt.show()


# In[99]:


final.groupby('day_week')['is_two'].mean()


# In[100]:


plt.figure(figsize=(12, 6))

# Строим столбчатую диаграмму с помощью pandas через plot(kind='bar')
final.groupby('day_week')['is_two'].mean().plot(
               kind='bar', # Тип графика - столбчатая диаграмма
               rot=0, # Градус вращения подписи по оси Х
               legend=False, # Выключаем легенду
               title=f'Распределение доли повторных заказов по дням недели')

plt.axhline(final.groupby('day_week')['is_two'].mean().mean(), color='red',
                linestyle='--', linewidth=1,)

# Настраиваем оформление графика
plt.xlabel('День недели(0 - пн, 1 - вт, 2 - ср, 3 - чт, 4- пт, 5 - сб, 6 - вск)')
plt.ylabel('Доля повторных заказов')
plt.grid()
# Выводим график
plt.show()


# день недели влияет на вероятность возврата клиента, но не сильно. Выше среднего показатели у пн, ср, сб - 60% и больше.
# 
# Сильно ниже остальных показатель вск - 57%

# ---
# 
# **Задача 4.3.2.** Изучите, как средний интервал между заказами влияет на удержание клиентов.
# 
# - Рассчитайте среднее время между заказами для двух групп пользователей:
#     - совершившие 2–4 заказа;
#     - совершившие 5 и более заказов.
# - Исследуйте, как средний интервал между заказами влияет на вероятность повторного заказа, и сделайте выводы.
# 
# ---
# 

# In[101]:


new_df_1 = final.loc[(final_df['is_two'] < 1)]


# In[102]:


new_df_2 = final.loc[(final_df['is_two'] > 0) & (final_df['is_five'] < 1)]


# In[103]:


new_df_5 = final.loc[(final_df['is_five'] > 0)]


# In[104]:


new_df_2['avg_days'].mean()


# In[105]:


new_df_5['avg_days'].mean()


# Для тех кто совершил 5 и более заказов среднее время между заказами менее 2 недель. Для тех кто сделал 2-4 заказа около 3 недель.

# ---
# 
# #### 4.4. Корреляционный анализ количества покупок и признаков пользователя
# 
# Изучите, какие характеристики первого заказа и профиля пользователя могут быть связаны с числом покупок. Для этого используйте универсальный коэффициент корреляции `phi_k`, который позволяет анализировать как числовые, так и категориальные признаки.
# 
# ---
# 
# **Задача 4.4.1:** Проведите корреляционный анализ:
# - Рассчитайте коэффициент корреляции `phi_k` между признаками профиля пользователя и числом заказов (`total_orders`). При необходимости используйте параметр `interval_cols` для определения интервальных данных.
# - Проанализируйте полученные результаты. Если полученные значения будут близки к нулю, проверьте разброс данных в `total_orders`. Такое возможно, когда в данных преобладает одно значение: в таком случае корреляционный анализ может показать отсутствие связей. Чтобы этого избежать, выделите сегменты пользователей по полю `total_orders`, а затем повторите корреляционный анализ. Выделите такие сегменты:
#     - 1 заказ;
#     - от 2 до 4 заказов;
#     - от 5 и выше.
# - Визуализируйте результат корреляции с помощью тепловой карты.
# - Ответьте на вопрос: какие признаки наиболее связаны с количеством заказов?
# 
# ---

# In[106]:


print(final.head())


# In[107]:


y=0
def create_is_y(x):
    if x<y:
        return 1
    return 0

y=2
final['is_one'] = final['order_count'].apply(create_is_y)


# In[108]:


print(final.head())


# In[109]:


correlation_matrix = final[['device_type_canonical', 'region_name', 'service_name', 'event_type_main', 'order_count','avg_revenue','avg_tickets','avg_days','day_week','is_one','is_two','is_five']].phik_matrix()
print('Корреляционная матрица с коэффициентом phi_k для переменной order_count')
correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count','is_one','is_two','is_five']].sort_values(by='order_count', ascending=False)


# In[110]:


# Строим тепловую карту
plt.figure(figsize=(2, 6))

data_heatmap = correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count','is_one','is_two','is_five']].sort_values(by='order_count', ascending=False)
sns.heatmap(data_heatmap,
            annot=True, # Отображаем численные значения в ячейках карты
            fmt='.2f', # Форматируем значения корреляции: два знака после точки
            cmap='coolwarm', # Устанавливаем цветовую гамму от красного (макс. значение) к синему
            linewidths=0.5, # Форматируем линию между ячейками карты
            cbar=False # Отключаем цветовую шкалу
           )

# Добавляем заголовок и подпись по оси Х
plt.title('Тепловая карта коэффициента phi_k \n для данных order_count')
plt.xlabel('Количество заказов')

# Выводим график
plt.show()


# In[111]:


correlation_matrix = new_df_2[['device_type_canonical', 'region_name', 'service_name', 'event_type_main', 'order_count','avg_revenue','avg_tickets','avg_days','day_week']].phik_matrix()
print('Корреляционная матрица с коэффициентом phi_k для переменной rating')
correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count']].sort_values(by='order_count', ascending=False)


# In[112]:


# Строим тепловую карту
plt.figure(figsize=(2, 6))

data_heatmap = correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count']].sort_values(by='order_count', ascending=False)
sns.heatmap(data_heatmap,
            annot=True, # Отображаем численные значения в ячейках карты
            fmt='.2f', # Форматируем значения корреляции: два знака после точки
            cmap='coolwarm', # Устанавливаем цветовую гамму от красного (макс. значение) к синему
            linewidths=0.5, # Форматируем линию между ячейками карты
            cbar=False # Отключаем цветовую шкалу
           )

# Добавляем заголовок и подпись по оси Х
plt.title('Тепловая карта коэффициента phi_k \n для данных order_count')
plt.xlabel('Количество заказов')

# Выводим график
plt.show()


# In[113]:


correlation_matrix = new_df_5[['device_type_canonical', 'region_name', 'service_name', 'event_type_main', 'order_count','avg_revenue','avg_tickets','avg_days','day_week']].phik_matrix()
print('Корреляционная матрица с коэффициентом phi_k для переменной rating')
correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count']].sort_values(by='order_count', ascending=False)


# In[114]:


# Строим тепловую карту
plt.figure(figsize=(2, 6))

data_heatmap = correlation_matrix.loc[correlation_matrix.index != 'order_count'][['order_count']].sort_values(by='order_count', ascending=False)
sns.heatmap(data_heatmap,
            annot=True, # Отображаем численные значения в ячейках карты
            fmt='.2f', # Форматируем значения корреляции: два знака после точки
            cmap='coolwarm', # Устанавливаем цветовую гамму от красного (макс. значение) к синему
            linewidths=0.5, # Форматируем линию между ячейками карты
            cbar=False # Отключаем цветовую шкалу
           )

# Добавляем заголовок и подпись по оси Х
plt.title('Тепловая карта коэффициента phi_k \n для данных order_count')
plt.xlabel('Количество заказов')

# Выводим график
plt.show()


# На то, совершил ли пользователь только один заказ сильно влияет среднее количество билетов в заказе(коэф кор 0,7) и средний чек(коэф кор 0,3).
# 
# На то, совершил ли пользователь 2-4 заказа сильно влияет среднее количество билетов в заказе(коэф кор 0,39) и количество дней между заказами(коэф кор 0,38).
# 
# На то, совершил ли пользователь 5 и более заказов сильно влияет количество дней между заказами(коэф кор 0,7).

# ### 5. Общий вывод и рекомендации
# 
# В конце проекта напишите общий вывод и рекомендации: расскажите заказчику, на что нужно обратить внимание. В выводах кратко укажите:
# 
# - **Информацию о данных**, с которыми вы работали, и то, как они были подготовлены: например, расскажите о фильтрации данных, переводе тенге в рубли, фильтрации выбросов.
# - **Основные результаты анализа.** Например, укажите:
#     - Сколько пользователей в выборке? Как распределены пользователи по числу заказов? Какие ещё статистические показатели вы подсчитали важным во время изучения данных?
#     - Какие признаки первого заказа связаны с возвратом пользователей?
#     - Как связаны средняя выручка и количество билетов в заказе с вероятностью повторных покупок?
#     - Какие временные характеристики влияют на удержание (день недели, интервалы между покупками)?
#     - Какие характеристики первого заказа и профиля пользователя могут быть связаны с числом покупок согласно результатам корреляционного анализа?
# - Дополните выводы информацией, которая покажется вам важной и интересной. Следите за общим объёмом выводов — они должны быть компактными и ёмкими.
# 
# В конце предложите заказчику рекомендации о том, как именно действовать в его ситуации. Например, укажите, на какие сегменты пользователей стоит обратить внимание в первую очередь, а какие нуждаются в дополнительных маркетинговых усилиях.

# **ВЫВОД**
# 
# а) **Подготовка**
# 
# Выгружены данные из базы SQL, общий начальный объем составил 14 столбцов и 290611 строк.
# 
# Выручка приведена к единой валюте 'rub', пересчет произведен по отношению к курсу валют на дату совершения сделки.
# 
# Проверены статистические данные, произошла фильтрация по 99 процентилю из-за сильных выбросов влияющих на общую статистику в столбцах по количеству приобретенных билетов и по количеству билетов в заказе.
# Эти данные составили 1% от общего числа строк.
# 
# Добавлены столбцы `is_two` — совершил ли пользователь 2 и более заказа;
# и `is_five`— совершил ли пользователь 5 и более заказов.
# 
# Произошла группировка по пользователям (всего пользователей 21847) и проверка на аномальные значения статичтических данных.
# В количестве заказов у некоторых пользователей обнаружено аномально большие значения. Принято решение отфильтровать по 95-му перцентилю.
# 
# Итоговый набор данных был сокращен на 5%, что составляет набор данных по 20 594 пользователям.
# 
# б) **Основные результаты анализа**
# 
# **Распределение пользователей:**
# 
# *По типам мероприятий*
# 
# На концерт и театр больше всего первых покупок.Концерт 44.4%, Театр 19.6%. 
# 
# *По типам устройств*
# 
# Мобильные заказы явный лидер - 83 %, ПК - 17 %
# 
# *По регионам*
# 
# Выделяются Каменевский регион 33% и Североярская область 17% остальные 6% и ниже
# 
# *По билетному оператору*
# 
# Пятерка лидеров
# 1) Билеты без проблем        23%
# 2) Мой билет                 14%
# 3) Лови билет!               13%
# 4) Билеты в руки             11%
# 5) Облачко                   10%
# 
# У других меньше 5%
# 
# **Доля возвратов пользователей по первому заказу:**
# 
# *По типам мероприятий*
# 
# Выше среднего показателя выставки 63% и театр 62%
# 
# *По типам устройств*
# 
# ПК - 62% против мобильных - 59%
# 
# *По регионам*
# 
# Особенно выделяются Широковская область 63%, Шанырский регион 65%, Светополянский округ 64%
# 
# *По билетному оператору*
# 
# Край билетов 63%, Дом культуры 63%
# 
# **Продуктовые гипотезы:**
# 
# Гипотеза 1. Тип мероприятия влияет на вероятность возврата на Яндекс Афишу: пользователи, которые совершили первый заказ на спортивные мероприятия, совершают повторный заказ чаще, чем пользователи, оформившие свой первый заказ на концерты.
# 
# **Не подтверждается**
# 
# спорт 53% вернувшихся(ниже среднего), концерты 60% вернувшихся(средний показатель)
# 
# Гипотеза 2. В регионах, где больше всего пользователей посещают мероприятия, выше доля повторных заказов, чем в менее активных регионах.
# 
# **Не подтверждается**
# 
# Топовые по посещению регионы Каменевский регион 60%, Североярская область 62%, в то время как в других менее активных есть показатели до 65% 
# 
# **Влияние средней выручки на возврат клиента**
# 
# Для тех кто вернулся 5 и более раз средний чек выше в области от 300 до 800 руб чем у тех кто сделал 1 заказ или 2-4 заказа, для этих данных распределение от 0 постепенно убывает.
# 
# **Влияние среднего количества билетов на возврат клиента**
# 
# Пользователи по сегментам распределены не равномерно, больше всего пользователей от 2 до 5 билетов в заказе.
# 
# Очень большой показатель повторных покупок у пользователей с билетами >=2 и <3 билетов в заказе.
# 
# Очень низкий показатель 5 и более.
# 
# **Влияние дня недели на возврат клиента**
# 
# день недели влияет на вероятность возврата клиента, но не сильно. Выше среднего показатели у пн, ср, сб - 60% и больше.
# 
# Сильно ниже остальных показатель вск - 57%
# 
# **Влияние интервала между покупками на возврат клиента**
# 
# Для тех кто совершил 5 и более заказов среднее время между заказами менее 2 недель. Для тех кто сделал 2-4 заказа около 3 недель.
# 
# **Анализ количества покупок и характеристики первого заказа**
# На то, совершил ли пользователь только один заказ сильно влияет среднее количество билетов в заказе(коэф кор 0,7) и средний чек(коэф кор 0,3).
# 
# На то, совершил ли пользователь 2-4 заказа сильно влияет среднее количество билетов в заказе(коэф кор 0,39) и количество дней между заказами(коэф кор 0,38).
# 
# На то, совершил ли пользователь 5 и более заказов сильно влияет количество дней между заказами(коэф кор 0,7).

# **РЕКОМЕНДАЦИИ**
# 
# В конце предложите заказчику рекомендации о том, как именно действовать в его ситуации. Например, укажите, на какие сегменты пользователей стоит обратить внимание в первую очередь, а какие нуждаются в дополнительных маркетинговых усилиях.
# 
# Необходимо обратить внимание на пользователей, которые делают первый заказ на концерты, с мобильных телефонов и из регионов с самым большим числом посетителей мероприятий - число покупателей выше чем у остальных категорий, но доля возвратов ниже.
# 
# Также необходимо пересмотреть всех топовых билетных операторов, у них число возвратных покупателей ниже среднего.
# 
# Средняя выручка для тех кто вернулся выше в диапазоне 300-800 руб.
# 
# Очень большой показатель повторных покупок у пользователей со среднем количеством билетов >=2 и <3 (видимо хорошие маркетинговые стратегии для пар)
# с 3 до 5 меньше, стоит сделать упор на семейные мероприятия
# 
# По дням недели больше всего покупают в пятницу, но показатель возвратов ниже среднего.
# 
# Чем чаще покупают, тем меньше времени проходит между заказами.
# На то совершил ли пользователь только один заказ сильно влияет количество билетов в заказе.
# 
# **КОРОТКО**
# 
# Слабые места по возрату - первый заказ на концерты, с мобильных телефонов, из регионов с самым большим числом посетителей мероприятий, топовые билетные операторы, покупки 3-5 билетов, покупки в пятницу.
# 
# Использование сильных сторон - Мероприятия для пар, средний чек 300-800 руб.

# ### 6. Финализация проекта и публикация в Git
# 
# Когда вы закончите анализировать данные, оформите проект, а затем опубликуйте его.
# 
# Выполните следующие действия:
# 
# 1. Создайте файл `.gitignore`. Добавьте в него все временные и чувствительные файлы, которые не должны попасть в репозиторий.
# 2. Сформируйте файл `requirements.txt`. Зафиксируйте все библиотеки, которые вы использовали в проекте.
# 3. Вынести все чувствительные данные (параметры подключения к базе) в `.env`файл.
# 4. Проверьте, что проект запускается и воспроизводим.
# 5. Загрузите проект в публичный репозиторий — например, на GitHub. Убедитесь, что все нужные файлы находятся в репозитории, исключая те, что в `.gitignore`. Ссылка на репозиторий понадобится для отправки проекта на проверку. Вставьте её в шаблон проекта в тетрадке Jupyter Notebook перед отправкой проекта на ревью.

# **Вставьте ссылку на проект в этой ячейке тетрадки перед отправкой проекта на ревью.**
